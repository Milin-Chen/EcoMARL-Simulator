# å¼ºåŒ–å­¦ä¹ çŽ¯å¢ƒè¯¦è§£

**RL Environment - GymæŽ¥å£ã€å¥–åŠ±å‡½æ•°ã€è§‚æµ‹ç©ºé—´**

---

## ðŸ“‹ ç›®å½•

- [1. çŽ¯å¢ƒæ¦‚è¿°](#1-çŽ¯å¢ƒæ¦‚è¿°)
- [2. è§‚æµ‹ç©ºé—´](#2-è§‚æµ‹ç©ºé—´)
- [3. åŠ¨ä½œç©ºé—´](#3-åŠ¨ä½œç©ºé—´)
- [4. å¥–åŠ±å‡½æ•°](#4-å¥–åŠ±å‡½æ•°)
- [5. ä½¿ç”¨ç¤ºä¾‹](#5-ä½¿ç”¨ç¤ºä¾‹)

---

## 1. çŽ¯å¢ƒæ¦‚è¿°

### 1.1 çŽ¯å¢ƒå±‚æ¬¡

```
rl_env/
â”œâ”€â”€ envs/
â”‚   â”œâ”€â”€ gym_env_enhanced.py          # åŸºç¡€çŽ¯å¢ƒ
â”‚   â”œâ”€â”€ gym_env_curriculum.py        # è¯¾ç¨‹å­¦ä¹ çŽ¯å¢ƒ
â”‚   â””â”€â”€ gym_env_curriculum_hpo.py    # HPOå¢žå¼ºçŽ¯å¢ƒ
â”‚
â”œâ”€â”€ rewards/
â”‚   â”œâ”€â”€ rewards_enhanced.py          # åŸºç¡€å¥–åŠ±V1
â”‚   â”œâ”€â”€ rewards_enhanced_v2.py       # å¢žå¼ºå¥–åŠ±V2
â”‚   â”œâ”€â”€ rewards_curriculum.py        # è¯¾ç¨‹å­¦ä¹ å¥–åŠ±
â”‚   â””â”€â”€ rewards_curriculum_hpo.py    # HPOå¢žå¼ºå¥–åŠ±
â”‚
â””â”€â”€ observations.py                  # è§‚æµ‹ç©ºé—´æå–å™¨
```

### 1.2 çŽ¯å¢ƒé€‰æ‹©

| çŽ¯å¢ƒ | ç”¨é€” | æŽ¨èåº¦ |
|------|------|--------|
| `CurriculumEcoMARLEnv` | è¯¾ç¨‹å­¦ä¹ è®­ç»ƒ | â­â­â­â­â­ |
| `CurriculumEcoMARLEnvHPO` | HPOå¢žå¼ºè®­ç»ƒ | â­â­â­â­ |
| `EnhancedEcoMARLEnv` | åŸºç¡€è®­ç»ƒ/æµ‹è¯• | â­â­â­ |

---

## 2. è§‚æµ‹ç©ºé—´

### 2.1 è§‚æµ‹å‘é‡ï¼ˆ16ç»´ï¼‰

```python
observation = [
    # è‡ªèº«çŠ¶æ€ (4ç»´)
    normalized_energy,      # [0, 1] èƒ½é‡ç™¾åˆ†æ¯”
    normalized_speed,       # [0, 1] é€Ÿåº¦ç™¾åˆ†æ¯”
    normalized_angular,     # [-1, 1] è§’é€Ÿåº¦å½’ä¸€åŒ–
    agent_type_encoding,    # 0.0=çŒŽäºº, 1.0=çŒŽç‰©

    # æœ€è¿‘ç›®æ ‡ (6ç»´)
    relative_x,            # [-1, 1] ç›¸å¯¹xåæ ‡
    relative_y,            # [-1, 1] ç›¸å¯¹yåæ ‡
    distance,              # [0, 1] è·ç¦»å½’ä¸€åŒ–
    angle_diff,            # [-1, 1] è§’åº¦å·®å¼‚
    target_speed,          # [0, 1] ç›®æ ‡é€Ÿåº¦
    target_type,           # 0.0=çŒŽäºº, 1.0=çŒŽç‰©

    # è§†é‡Žå†…ç»Ÿè®¡ (6ç»´)
    num_hunters_visible,   # [0, 1] å¯è§çŒŽäººæ•°é‡å½’ä¸€åŒ–
    num_preys_visible,     # [0, 1] å¯è§çŒŽç‰©æ•°é‡å½’ä¸€åŒ–
    avg_hunter_distance,   # [0, 1] å¹³å‡çŒŽäººè·ç¦»
    avg_prey_distance,     # [0, 1] å¹³å‡çŒŽç‰©è·ç¦»
    nearest_hunter_dist,   # [0, 1] æœ€è¿‘çŒŽäººè·ç¦»
    nearest_prey_dist,     # [0, 1] æœ€è¿‘çŒŽç‰©è·ç¦»
]
```

### 2.2 è§‚æµ‹æå–

**å®žçŽ°**: `rl_env/observations.py`

```python
from rl_env import ObservationSpace

obs_space = ObservationSpace(agent_config)
obs = obs_space.get_observation(entity, all_entities)
# obs.shape = (16,)
```

---

## 3. åŠ¨ä½œç©ºé—´

### 3.1 è¿žç»­åŠ¨ä½œï¼ˆ2ç»´ï¼‰

```python
action = [
    speed_delta,          # [-1, 1] åŠ é€Ÿ/å‡é€Ÿ
    angular_velocity_delta  # [-1, 1] å·¦è½¬/å³è½¬
]
```

### 3.2 åŠ¨ä½œåº”ç”¨

```python
# é€Ÿåº¦æ›´æ–°
entity.speed = clip(
    entity.speed + speed_delta * SPEED_INCREMENT,
    0.0,
    MAX_SPEED
)

# è§’é€Ÿåº¦æ›´æ–°
entity.angular_velocity = clip(
    entity.angular_velocity + angular_delta * ANGULAR_INCREMENT,
    -MAX_ANGULAR,
    MAX_ANGULAR
)
```

---

## 4. å¥–åŠ±å‡½æ•°

### 4.1 å¥–åŠ±ä½“ç³»

#### çŒŽäººå¥–åŠ±ï¼ˆStage1HunterRewardï¼‰

```python
æ€»å¥–åŠ± = å­˜æ´»å¥–åŠ± + è¿½å‡»å¥–åŠ± + æ•èŽ·å¥–åŠ± + èƒ½é‡æƒ©ç½š
```

**åˆ†é¡¹**:
- **å­˜æ´»å¥–åŠ±**: `+1.0/æ­¥` (ä¿æŒå­˜æ´»)
- **è¿½å‡»å¥–åŠ±**: `è·ç¦»å‡å°‘ * 2.0` (æŽ¥è¿‘çŒŽç‰©)
- **æ•èŽ·å¥–åŠ±**: `+50.0` (æˆåŠŸæ•èŽ·)
- **èƒ½é‡æƒ©ç½š**: `-èƒ½é‡æ¶ˆè€— * 0.1` (é¼“åŠ±èŠ‚èƒ½)

**æŒç»­è¿½å‡»åŠ æˆ** (HPOç‰ˆæœ¬):
```python
chase_multiplier = 1.0 + 2.0 * (chase_streak / 10)
# chase_streak=10 â†’ 3.0xå€æ•°
```

#### çŒŽç‰©å¥–åŠ±ï¼ˆStage3PreyRewardï¼‰

```python
æ€»å¥–åŠ± = å­˜æ´»å¥–åŠ± + é€ƒè·‘å¥–åŠ± + è¢«æ•æƒ©ç½š
```

**åˆ†é¡¹**:
- **å­˜æ´»å¥–åŠ±**: `+2.0/æ­¥` (ä¿æŒå­˜æ´»)
- **é€ƒè·‘å¥–åŠ±**: `è·ç¦»å¢žåŠ  * 3.0` (è¿œç¦»çŒŽäºº)
- **è¢«æ•æƒ©ç½š**: `-100.0` (è¢«æ•èŽ·)

**æŒç»­é€ƒè·‘åŠ æˆ** (HPOç‰ˆæœ¬):
```python
escape_multiplier = 1.0 + 2.0 * (escape_streak / 10)
# escape_streak=10 â†’ 3.0xå€æ•°
```

### 4.2 è¯¾ç¨‹å­¦ä¹ å¥–åŠ±

| é˜¶æ®µ | å¥–åŠ±å‡½æ•° | ç‰¹ç‚¹ |
|------|---------|------|
| Stage 1 | Stage1HunterReward | ç®€å•è¿½å‡»ï¼Œé™æ­¢ç›®æ ‡ |
| Stage 2 | Stage2HunterReward | é¢„æµ‹æ‹¦æˆªï¼Œç§»åŠ¨ç›®æ ‡ |
| Stage 3 | Stage3PreyReward | é€ƒé¿ç­–ç•¥ |
| Stage 4 | Stage4JointReward | å¹³è¡¡å¯¹æŠ— |

### 4.3 è‡ªå®šä¹‰å¥–åŠ±

```python
from rl_env import Stage1HunterReward

class MyReward(Stage1HunterReward):
    def compute_reward(self, hunter, world_state, prev_state):
        # è°ƒç”¨çˆ¶ç±»åŸºç¡€å¥–åŠ±
        base_reward = super().compute_reward(hunter, world_state, prev_state)

        # è‡ªå®šä¹‰å¥–åŠ±ï¼šå¥–åŠ±é«˜èƒ½é‡
        energy_bonus = 0.0
        if hunter.energy > 80:
            energy_bonus = 2.0

        # æƒ©ç½šä½Žé€Ÿåº¦ï¼ˆé¼“åŠ±æ¿€è¿›ï¼‰
        speed_penalty = 0.0
        if hunter.speed < 20:
            speed_penalty = -1.0

        return base_reward + energy_bonus + speed_penalty
```

---

## 5. ä½¿ç”¨ç¤ºä¾‹

### 5.1 åŸºç¡€è®­ç»ƒ

```python
from rl_env import CurriculumEcoMARLEnv
from stable_baselines3 import PPO

# åˆ›å»ºçŽ¯å¢ƒ
env = CurriculumEcoMARLEnv(
    stage="stage1",
    n_hunters=3,
    n_prey=6,
)

# åˆ›å»ºPPOæ¨¡åž‹
model = PPO("MlpPolicy", env, verbose=1)

# è®­ç»ƒ
model.learn(total_timesteps=50000)

# ä¿å­˜
model.save("my_hunter_model")
```

### 5.2 çŽ¯å¢ƒäº¤äº’

```python
# é‡ç½®çŽ¯å¢ƒ
obs = env.reset()
# obs: Dict[agent_id, observation_vector]

done = False
while not done:
    # éšæœºåŠ¨ä½œï¼ˆæˆ–ä½¿ç”¨æ¨¡åž‹ï¼‰
    actions = {
        agent_id: env.action_space.sample()
        for agent_id in obs.keys()
    }

    # æ‰§è¡Œæ­¥éª¤
    obs, rewards, dones, info = env.step(actions)

    # æ£€æŸ¥ç»ˆæ­¢
    done = all(dones.values())

env.close()
```

### 5.3 ä½¿ç”¨è®­ç»ƒå¥½çš„æ¨¡åž‹

```python
from stable_baselines3 import PPO

# åŠ è½½æ¨¡åž‹
hunter_model = PPO.load("curriculum_models/stage2_hunter_final.zip")

# æŽ¨ç†
obs = env.reset()
for _ in range(1000):
    # åªæŽ§åˆ¶çŒŽäºº
    hunter_actions = {}
    for agent_id, agent_obs in obs.items():
        if "hunter" in agent_id:
            action, _ = hunter_model.predict(agent_obs, deterministic=True)
            hunter_actions[agent_id] = action

    obs, rewards, dones, info = env.step(hunter_actions)
```

---

## ðŸ“š ç›¸å…³æ–‡æ¡£

- [è®­ç»ƒç³»ç»Ÿ](TRAINING_SYSTEM.md) - å¦‚ä½•ä½¿ç”¨çŽ¯å¢ƒè®­ç»ƒ
- [æ ¸å¿ƒæ¨¡å—](CORE_MODULES.md) - åº•å±‚ç‰©ç†å¼•æ“Ž
- [é…ç½®ç³»ç»Ÿ](CONFIGURATION.md) - çŽ¯å¢ƒå‚æ•°é…ç½®

---

**æŽŒæ¡RLçŽ¯å¢ƒï¼Œæ‰“é€ æ™ºèƒ½å†³ç­–ï¼** ðŸ¤–
