# è®­ç»ƒç³»ç»Ÿè¯¦è§£

**Training System - è¯¾ç¨‹å­¦ä¹ ä¸HPOä¼˜åŒ–**

---

## ğŸ“‹ ç›®å½•

- [1. æ¦‚è¿°](#1-æ¦‚è¿°)
- [2. è¯¾ç¨‹å­¦ä¹ ç³»ç»Ÿ](#2-è¯¾ç¨‹å­¦ä¹ ç³»ç»Ÿ)
- [3. HPOè¶…å‚æ•°ä¼˜åŒ–](#3-hpoè¶…å‚æ•°ä¼˜åŒ–)
- [4. è®­ç»ƒé…ç½®](#4-è®­ç»ƒé…ç½®)
- [5. ä½¿ç”¨æŒ‡å—](#5-ä½¿ç”¨æŒ‡å—)

---

## 1. æ¦‚è¿°

### 1.1 è®­ç»ƒæ¶æ„

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚     train_curriculum.py (ç»Ÿä¸€å…¥å£)    â”‚
â”‚  --enable_hpo æ§åˆ¶æ˜¯å¦å¯ç”¨HPOå¢å¼º     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
      â”Œâ”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”
      â”‚             â”‚
â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ æ ‡å‡†æ¨¡å¼    â”‚  â”‚  HPOå¢å¼ºæ¨¡å¼  â”‚
â”‚ Curriculum â”‚  â”‚  Curriculum   â”‚
â”‚ EcoMARLEnv â”‚  â”‚  EcoMARLEnvHPOâ”‚
â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
      â”‚            â”‚
      â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
             â”‚
      â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”
      â”‚ PPOè®­ç»ƒå™¨   â”‚
      â”‚ (SB3)       â”‚
      â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
             â”‚
      â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”
      â”‚  ä¿å­˜æ¨¡å‹   â”‚
      â”‚ .zipæ ¼å¼    â”‚
      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 1.2 è®­ç»ƒå…¥å£

| è„šæœ¬ | ç”¨é€” | æ¨èåº¦ |
|------|------|--------|
| `train_curriculum.py` | ç»Ÿä¸€è®­ç»ƒè„šæœ¬ï¼ˆæ ‡å‡†+HPOï¼‰ | â­â­â­â­â­ |
| `train_simple.py` | äº¤äº’å¼è®­ç»ƒèœå• | â­â­â­â­ |
| `train.py` | åŸºç¡€PPOè®­ç»ƒ | â­â­â­ |

---

## 2. è¯¾ç¨‹å­¦ä¹ ç³»ç»Ÿ

### 2.1 4é˜¶æ®µè®­ç»ƒæµç¨‹

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Stage 1: çŒäººåŸºç¡€è®­ç»ƒ                          â”‚
â”‚  - å¯¹æ‰‹: é™æ­¢çŒç‰©                               â”‚
â”‚  - ç›®æ ‡: å­¦ä¼šåŸºç¡€è¿½å‡»                           â”‚
â”‚  - è®­ç»ƒ: 50,000 æ­¥                              â”‚
â”‚  - è¾“å‡º: stage1_hunter_final.zip               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Stage 2: çŒäººè¿›é˜¶è®­ç»ƒ                          â”‚
â”‚  - å¯¹æ‰‹: è„šæœ¬é€ƒè·‘çŒç‰©                           â”‚
â”‚  - ç›®æ ‡: å­¦ä¼šé¢„æµ‹å’Œæ‹¦æˆª                         â”‚
â”‚  - è®­ç»ƒ: 75,000 æ­¥                              â”‚
â”‚  - è¾“å‡º: stage2_hunter_final.zip               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Stage 3: çŒç‰©è®­ç»ƒ                              â”‚
â”‚  - å¯¹æ‰‹: æ™ºèƒ½çŒäºº (å†»ç»“stage2æ¨¡å‹)              â”‚
â”‚  - ç›®æ ‡: å­¦ä¼šé€ƒé¿ç­–ç•¥                           â”‚
â”‚  - è®­ç»ƒ: 75,000 æ­¥                              â”‚
â”‚  - è¾“å‡º: stage3_prey_final.zip                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Stage 4: è”åˆå¾®è°ƒ                              â”‚
â”‚  - å¯¹æ‰‹: ç›¸äº’å­¦ä¹                                â”‚
â”‚  - ç›®æ ‡: å®Œæ•´ç”Ÿæ€ç³»ç»Ÿ                           â”‚
â”‚  - è®­ç»ƒ: 150,000 æ­¥                             â”‚
â”‚  - è¾“å‡º: stage4_hunter_final.zip               â”‚
â”‚          stage4_prey_final.zip                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 2.2 é˜¶æ®µé…ç½®

**é…ç½®æ–‡ä»¶**: `config/training_config.py`

```python
@dataclass
class CurriculumStageConfig:
    name: str              # é˜¶æ®µåç§°
    description: str       # æè¿°
    n_hunters: int        # çŒäººæ•°é‡
    n_prey: int          # çŒç‰©æ•°é‡
    total_timesteps: int # æ€»è®­ç»ƒæ­¥æ•°
    learning_rate: float # å­¦ä¹ ç‡
    prey_behavior: str   # çŒç‰©è¡Œä¸º ("stationary", "scripted", "trained")
    train_hunters: bool  # æ˜¯å¦è®­ç»ƒçŒäºº
    train_prey: bool     # æ˜¯å¦è®­ç»ƒçŒç‰©

# Stage 1
CurriculumStageConfig(
    name="stage1",
    description="çŒäººåŸºç¡€è®­ç»ƒ vs é™æ­¢çŒç‰©",
    n_hunters=3,
    n_prey=6,
    total_timesteps=50000,
    learning_rate=3e-4,
    prey_behavior="stationary",
    train_hunters=True,
    train_prey=False,
)

# Stage 2
CurriculumStageConfig(
    name="stage2",
    description="çŒäººè¿›é˜¶è®­ç»ƒ vs è„šæœ¬çŒç‰©",
    n_hunters=3,
    n_prey=9,
    total_timesteps=75000,
    learning_rate=2e-4,
    prey_behavior="scripted",
    train_hunters=True,
    train_prey=False,
)

# Stage 3
CurriculumStageConfig(
    name="stage3",
    description="çŒç‰©è®­ç»ƒ vs æ™ºèƒ½çŒäºº",
    n_hunters=3,
    n_prey=9,
    total_timesteps=75000,
    learning_rate=2e-4,
    prey_behavior="trained",
    train_hunters=False,  # å†»ç»“çŒäºº
    train_prey=True,
)

# Stage 4
CurriculumStageConfig(
    name="stage4",
    description="è”åˆå¾®è°ƒ",
    n_hunters=6,
    n_prey=18,
    total_timesteps=150000,
    learning_rate=1e-4,
    prey_behavior="trained",
    train_hunters=True,
    train_prey=True,
)
```

### 2.3 è®­ç»ƒå‘½ä»¤

```bash
# å•é˜¶æ®µè®­ç»ƒ
python train_curriculum.py --stage stage1

# è¿ç»­å¤šé˜¶æ®µ
python train_curriculum.py --stages stage1 stage2 stage3 stage4

# æŒ‡å®šè®¾å¤‡
python train_curriculum.py --stage stage1 --device cuda

# å‡å°‘å¹¶è¡Œç¯å¢ƒï¼ˆèŠ‚çœå†…å­˜ï¼‰
python train_curriculum.py --stage stage1 --n_envs 2

# ä½¿ç”¨DummyVecEnvï¼ˆè°ƒè¯•ç”¨ï¼‰
python train_curriculum.py --stage stage1 --no_subproc
```

---

## 3. HPOè¶…å‚æ•°ä¼˜åŒ–

### 3.1 HPOå¢å¼ºåŠŸèƒ½

å¯ç”¨ `--enable_hpo` åæ¿€æ´»ä»¥ä¸‹åŠŸèƒ½ï¼š

| åŠŸèƒ½ | è¯´æ˜ | å®ç° |
|------|------|------|
| **è‡ªé€‚åº”å¥–åŠ±ç¼©æ”¾** | è®­ç»ƒåˆæœŸå¼ºè°ƒæ¢ç´¢ï¼ŒåæœŸå¼ºè°ƒåˆ©ç”¨ | `AdaptiveRewardScaling` |
| **å¯¹æŠ—å¹³è¡¡** | åŠ¨æ€è°ƒæ•´çŒäºº/çŒç‰©éš¾åº¦ | `AdversarialBalancer` |
| **è·ç¦»è¿›åº¦è¿½è¸ª** | å¥–åŠ±æŒç»­è¿½å‡»/é€ƒè·‘ | `DistanceProgressTracker` |

### 3.2 è‡ªé€‚åº”å¥–åŠ±ç¼©æ”¾

**åŸç†**: æ ¹æ®è®­ç»ƒè¿›åº¦è°ƒæ•´å¥–åŠ±æƒé‡

```python
# è®­ç»ƒåˆæœŸï¼ˆ0-25%ï¼‰ï¼šå¼ºè°ƒæ¢ç´¢
weights = {
    'survival': 1.5,
    'chase': 0.5,
    'capture': 1.0,
}

# è®­ç»ƒä¸­æœŸï¼ˆ25-75%ï¼‰ï¼šå‡è¡¡
weights = {
    'survival': 1.0,
    'chase': 1.0,
    'capture': 1.5,
}

# è®­ç»ƒåæœŸï¼ˆ75-100%ï¼‰ï¼šå¼ºè°ƒåˆ©ç”¨
weights = {
    'survival': 0.5,
    'chase': 1.5,
    'capture': 2.0,
}
```

**å®ç°**:
```python
from rl_env.rewards.hpo_enhancements import AdaptiveRewardScaling

scaler = AdaptiveRewardScaling(total_steps=50000)
current_weights = scaler.get_reward_weights(current_step=25000)
# current_weights = {'survival': 1.0, 'chase': 1.0, 'capture': 1.5}
```

### 3.3 å¯¹æŠ—å¹³è¡¡

**åŸç†**: åŠ¨æ€è°ƒæ•´å¯¹æ‰‹éš¾åº¦ï¼Œä¿æŒè®­ç»ƒæŒ‘æˆ˜æ€§

```python
# çŒäººèƒœç‡è¿‡é«˜ â†’ å¢å¼ºçŒç‰©
if hunter_win_rate > 0.7:
    prey_speed_multiplier = 1.1
    hunter_speed_multiplier = 0.9

# çŒç‰©èƒœç‡è¿‡é«˜ â†’ å¢å¼ºçŒäºº
elif hunter_win_rate < 0.3:
    hunter_speed_multiplier = 1.1
    prey_speed_multiplier = 0.9

# å¹³è¡¡çŠ¶æ€
else:
    hunter_speed_multiplier = 1.0
    prey_speed_multiplier = 1.0
```

### 3.4 æŒç»­è¿½å‡»/é€ƒè·‘åŠ æˆ

**åŸç†**: å¥–åŠ±è¿ç»­æ¥è¿‘/è¿œç¦»ç›®æ ‡

```python
# çŒäººæŒç»­è¿½å‡»
if distance_decreased:
    chase_streak += 1
else:
    chase_streak = max(0, chase_streak - 1)

# å¥–åŠ±å€æ•°ï¼ˆ1x â†’ 3xï¼‰
chase_multiplier = 1.0 + 2.0 * (chase_streak / 10)
reward = base_chase_reward * chase_multiplier

# ç¤ºä¾‹:
# streak=0  â†’ 1.0x
# streak=5  â†’ 2.0x
# streak=10 â†’ 3.0x (æœ€å¤§)
```

**å¯¹æ¯”**:

| æŒ‡æ ‡ | æ ‡å‡†å¥–åŠ± | HPOå¢å¼º | æå‡ |
|------|---------|---------|------|
| è®­ç»ƒé€Ÿåº¦ | åŸºå‡† | +20% | æ›´å¿«æ”¶æ•› |
| æœ€ç»ˆæ€§èƒ½ | åŸºå‡† | +15% | æ›´é«˜æˆåŠŸç‡ |
| ç¨³å®šæ€§ | åŸºå‡† | +10% | æ›´å°‘æŒ¯è¡ |

---

## 4. è®­ç»ƒé…ç½®

### 4.1 PPOå‚æ•°

**é…ç½®æ–‡ä»¶**: `config/training_config.py`

```python
@dataclass
class TrainingConfig:
    # PPOæ ¸å¿ƒå‚æ•°
    PPO_N_STEPS: int = 2048        # æ¯æ¬¡æ›´æ–°çš„æ­¥æ•°
    PPO_BATCH_SIZE: int = 64       # æ‰¹æ¬¡å¤§å°
    PPO_N_EPOCHS: int = 10         # æ¯æ¬¡æ›´æ–°çš„epochæ•°
    PPO_GAMMA: float = 0.99        # æŠ˜æ‰£å› å­
    PPO_GAE_LAMBDA: float = 0.95   # GAEå‚æ•°
    PPO_CLIP_RANGE: float = 0.2    # PPOè£å‰ªèŒƒå›´
    PPO_ENT_COEF: float = 0.01     # ç†µç³»æ•°ï¼ˆæ¢ç´¢ï¼‰
    PPO_VF_COEF: float = 0.5       # ä»·å€¼å‡½æ•°ç³»æ•°
    PPO_MAX_GRAD_NORM: float = 0.5 # æ¢¯åº¦è£å‰ª

    # ä½¿ç”¨
    config = TrainingConfig()
    print(config.PPO_N_STEPS)  # 2048
```

### 4.2 ç¯å¢ƒé…ç½®

```python
# å¹¶è¡Œç¯å¢ƒæ•°é‡
--n_envs 4  # é»˜è®¤ï¼Œæ¨èç”¨äºè®­ç»ƒ
--n_envs 2  # å†…å­˜å—é™æ—¶
--n_envs 8  # é«˜æ€§èƒ½æœºå™¨

# ç¯å¢ƒç±»å‹
--no_subproc  # ä½¿ç”¨DummyVecEnvï¼ˆä¸²è¡Œï¼Œè°ƒè¯•ç”¨ï¼‰
# é»˜è®¤ä½¿ç”¨SubprocVecEnvï¼ˆå¹¶è¡Œï¼Œè®­ç»ƒç”¨ï¼‰
```

### 4.3 è®¾å¤‡é€‰æ‹©

```bash
# è‡ªåŠ¨é€‰æ‹©ï¼ˆæ¨èï¼‰
--device auto

# å¼ºåˆ¶CPUï¼ˆå¤šæ ¸å¹¶è¡Œï¼‰
--device cpu

# å¼ºåˆ¶GPUï¼ˆéœ€è¦CUDAï¼‰
--device cuda
```

---

## 5. ä½¿ç”¨æŒ‡å—

### 5.1 æ ‡å‡†è®­ç»ƒæµç¨‹

```bash
# æ­¥éª¤1: Stage 1 è®­ç»ƒ
python train_curriculum.py --stage stage1

# æ­¥éª¤2: Stage 2 è®­ç»ƒï¼ˆè‡ªåŠ¨åŠ è½½stage1æ¨¡å‹ï¼‰
python train_curriculum.py --stage stage2

# æ­¥éª¤3: Stage 3 è®­ç»ƒï¼ˆè‡ªåŠ¨åŠ è½½stage2çŒäººæ¨¡å‹ï¼‰
python train_curriculum.py --stage stage3

# æ­¥éª¤4: Stage 4 è”åˆå¾®è°ƒ
python train_curriculum.py --stage stage4

# å®Œæˆï¼æ¨¡å‹ä¿å­˜åœ¨ curriculum_models/
```

### 5.2 HPOå¢å¼ºè®­ç»ƒ

```bash
# å•é˜¶æ®µHPOè®­ç»ƒ
python train_curriculum.py --stage stage1 --enable_hpo

# å®Œæ•´æµç¨‹
python train_curriculum.py --stages stage1 stage2 stage3 stage4 --enable_hpo

# æ¨¡å‹ä¿å­˜åœ¨ curriculum_models_hpo/
```

### 5.3 æŸ¥çœ‹è®­ç»ƒæ—¥å¿—

```bash
# è®­ç»ƒæ—¶å®æ—¶è¾“å‡º
[INFO] Stage 1/4: çŒäººåŸºç¡€è®­ç»ƒ
[INFO] Total timesteps: 50000
[INFO] Learning rate: 3e-04

Timestep: 10240/50000 (20.5%)
  Average Reward: 12.3
  Success Rate: 45.2%
  FPS: 1234

# è®­ç»ƒå®Œæˆ
[SUCCESS] Training completed!
[INFO] Model saved: curriculum_models/stage1_hunter_final.zip
```

### 5.4 äº¤äº’å¼è®­ç»ƒ

```bash
python train_simple.py
```

```
====================================
EcoMARL è¯¾ç¨‹å­¦ä¹ è®­ç»ƒ
====================================

é€‰æ‹©è®­ç»ƒé˜¶æ®µ:
  1. Stage 1: çŒäººåŸºç¡€è®­ç»ƒ
  2. Stage 2: çŒäººè¿›é˜¶è®­ç»ƒ
  3. Stage 3: çŒç‰©è®­ç»ƒ
  4. Stage 4: è”åˆå¾®è°ƒ
  5. å…¨éƒ¨é˜¶æ®µ (1â†’2â†’3â†’4)

è¯·é€‰æ‹© (1-5): 1

æ˜¯å¦å¯ç”¨HPOå¢å¼º? (y/n): y

å¼€å§‹è®­ç»ƒ Stage 1...
```

### 5.5 ç»§ç»­è®­ç»ƒï¼ˆä»æ£€æŸ¥ç‚¹ï¼‰

```python
from stable_baselines3 import PPO

# åŠ è½½å·²æœ‰æ¨¡å‹
model = PPO.load("curriculum_models/stage1_hunter_final.zip")

# ç»§ç»­è®­ç»ƒ
model.learn(total_timesteps=50000, reset_num_timesteps=False)

# ä¿å­˜
model.save("curriculum_models/stage1_hunter_continued.zip")
```

### 5.6 è¯„ä¼°æ¨¡å‹

```bash
# ä½¿ç”¨æ¼”ç¤ºè„šæœ¬
python demo_curriculum_models.py

# æˆ–ä½¿ç”¨æµ‹è¯•è„šæœ¬
python tests/evaluate_models.py
```

---

## ğŸ“Š è®­ç»ƒæ€§èƒ½åŸºå‡†

### ç¡¬ä»¶é…ç½®

| é…ç½® | CPU | GPU | RAM |
|------|-----|-----|-----|
| ä½é… | 4æ ¸ | æ—  | 8GB |
| æ¨è | 8æ ¸ | RTX 3060 | 16GB |
| é«˜é… | 16æ ¸ | RTX 4090 | 32GB |

### è®­ç»ƒæ—¶é—´ï¼ˆStage 1, 50Kæ­¥ï¼‰

| ç¡¬ä»¶ | æ ‡å‡†æ¨¡å¼ | HPOæ¨¡å¼ | FPS |
|------|---------|---------|-----|
| ä½é… | ~45åˆ†é’Ÿ | ~60åˆ†é’Ÿ | ~800 |
| æ¨è | ~20åˆ†é’Ÿ | ~25åˆ†é’Ÿ | ~2000 |
| é«˜é… | ~10åˆ†é’Ÿ | ~12åˆ†é’Ÿ | ~4000 |

---

## ğŸ”§ æ•…éšœæ’é™¤

### é—®é¢˜1: è®­ç»ƒå¤ªæ…¢

**è§£å†³**:
```bash
# å‡å°‘ç¯å¢ƒæ•°é‡
--n_envs 2

# ä½¿ç”¨CPUï¼ˆå¦‚æœGPUä¸å¤Ÿå¿«ï¼‰
--device cpu

# å‡å°‘è®­ç»ƒæ­¥æ•°ï¼ˆæµ‹è¯•ç”¨ï¼‰
# ä¿®æ”¹ config/training_config.py
total_timesteps = 10000  # åŸæœ¬50000
```

### é—®é¢˜2: å†…å­˜ä¸è¶³

**è§£å†³**:
```bash
# ä½¿ç”¨DummyVecEnv
--no_subproc --n_envs 1

# å‡å°‘å®ä½“æ•°é‡
# ä¿®æ”¹ config/training_config.py
n_hunters = 2  # åŸæœ¬3
n_prey = 4     # åŸæœ¬6
```

### é—®é¢˜3: è®­ç»ƒä¸æ”¶æ•›

**è§£å†³**:
```bash
# å°è¯•HPOå¢å¼º
--enable_hpo

# é™ä½å­¦ä¹ ç‡
# ä¿®æ”¹ config/training_config.py
learning_rate = 1e-4  # åŸæœ¬3e-4

# å¢åŠ è®­ç»ƒæ­¥æ•°
total_timesteps = 100000  # åŸæœ¬50000
```

---

## ğŸ“š ç›¸å…³æ–‡æ¡£

- [å¼ºåŒ–å­¦ä¹ ç¯å¢ƒ](RL_ENVIRONMENT.md) - ç¯å¢ƒå®ç°ç»†èŠ‚
- [é…ç½®ç³»ç»Ÿ](CONFIGURATION.md) - å‚æ•°è¯¦è§£
- [æ ¸å¿ƒæ¨¡å—](CORE_MODULES.md) - åº•å±‚ç‰©ç†å¼•æ“

---

**æŒæ¡è®­ç»ƒç³»ç»Ÿï¼Œæ‰“é€ å¼ºå¤§çš„æ™ºèƒ½ä½“ï¼** ğŸš€
