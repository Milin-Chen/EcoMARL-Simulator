"""
åˆ†é˜¶æ®µè¯¾ç¨‹å­¦ä¹ è®­ç»ƒè„šæœ¬ (ç»Ÿä¸€ç‰ˆæœ¬)
Curriculum Learning Training Script (Unified Version)

åŠŸèƒ½:
- é»˜è®¤ä½¿ç”¨æ ‡å‡†è¯¾ç¨‹å­¦ä¹  (CurriculumEcoMARLEnv)
- å¯é€‰å¯ç”¨HPOå¢å¼º (--enable_hpo)
- ç»Ÿä¸€ä½¿ç”¨ config/training_config.py ä¸­çš„å‚æ•°
- ç®€åŒ–ä»£ç ï¼Œç§»é™¤ç¡¬ç¼–ç å‚æ•°

4ä¸ªé˜¶æ®µ:
1. Stage 1: çŒäºº vs é™æ­¢çŒç‰©
2. Stage 2: çŒäºº vs è„šæœ¬çŒç‰©
3. Stage 3: å†»ç»“çŒäºº, è®­ç»ƒçŒç‰©
4. Stage 4: è”åˆå¾®è°ƒ
"""

# CRITICAL: These must be set before ANY other imports
import os
os.environ['OBJC_DISABLE_INITIALIZE_FORK_SAFETY'] = 'YES'

import multiprocessing
import sys

# Force fork method instead of spawn to avoid numpy issues on macOS
# This MUST be done before importing any packages that use multiprocessing
if 'torch' not in sys.modules and 'numpy' not in sys.modules:
    try:
        multiprocessing.set_start_method('fork', force=True)
    except RuntimeError:
        pass  # Already set

import argparse
from pathlib import Path
from datetime import datetime

import numpy as np
import torch
from stable_baselines3 import PPO
from stable_baselines3.common.vec_env import DummyVecEnv, SubprocVecEnv

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
PROJECT_ROOT = Path(__file__).parent
sys.path.insert(0, str(PROJECT_ROOT))

from config.agent_config import AgentConfig
from config.env_config import EnvConfig
from config.training_config import TrainingConfig
from rl_env import (
    EnhancedEcoMARLEnv,
    CurriculumEcoMARLEnv,
    CurriculumEcoMARLEnvHPO,
    create_logger_callback,
)


# ===== ç¯å¢ƒåˆ›å»º =====


def make_vec_env(
    stage: str,
    model_dir: str,
    n_envs: int = 4,
    use_subproc: bool = True,
    enable_hpo: bool = False,
):
    """
    åˆ›å»ºå¹¶è¡Œå‘é‡åŒ–ç¯å¢ƒ

    Args:
        stage: è®­ç»ƒé˜¶æ®µ ("stage1", "stage2", "stage3", "stage4")
        model_dir: æ¨¡å‹ç›®å½•
        n_envs: å¹¶è¡Œç¯å¢ƒæ•°é‡
        use_subproc: æ˜¯å¦ä½¿ç”¨å¤šè¿›ç¨‹
        enable_hpo: æ˜¯å¦å¯ç”¨HPOå¢å¼º
    """
    # è·å–é˜¶æ®µé…ç½®
    stage_config = TrainingConfig.get_stage_config(stage)
    total_steps = stage_config.total_timesteps

    def make_env():
        # ç›´æ¥ä½¿ç”¨é»˜è®¤é…ç½® (æ— éœ€ç¡¬ç¼–ç )
        agent_config = AgentConfig()
        env_config = EnvConfig()

        # åˆ›å»ºåŸºç¡€ç¯å¢ƒ
        base_env = EnhancedEcoMARLEnv(
            agent_config=agent_config,
            env_config=env_config,
            n_hunters=stage_config.n_hunters,
            n_prey=stage_config.n_prey,
            max_steps=1000,
            use_v2_rewards=True,  # é»˜è®¤ä½¿ç”¨V2å¥–åŠ±
        )

        # åˆ›å»ºè¯¾ç¨‹å­¦ä¹ ç¯å¢ƒ (é€‰æ‹©HPOæˆ–æ ‡å‡†ç‰ˆæœ¬)
        if enable_hpo:
            env = CurriculumEcoMARLEnvHPO(
                base_env=base_env,
                stage=stage,
                enable_hpo=True,
                total_steps=total_steps,
            )
        else:
            env = CurriculumEcoMARLEnv(
                base_env=base_env,
                stage=stage,
            )

        return env

    # åˆ›å»ºnä¸ªç¯å¢ƒ
    env_fns = [make_env for _ in range(n_envs)]

    if use_subproc and n_envs > 1:
        vec_env = SubprocVecEnv(env_fns)
        print(f"âœ“ åˆ›å»º {n_envs} ä¸ªå¹¶è¡Œç¯å¢ƒ (SubprocVecEnv - å¤šè¿›ç¨‹)")
    else:
        vec_env = DummyVecEnv(env_fns)
        env_type = "å•ç¯å¢ƒ" if n_envs == 1 else f"{n_envs}ä¸ªç¯å¢ƒ (å•è¿›ç¨‹)"
        print(f"âœ“ åˆ›å»º {env_type} (DummyVecEnv)")

    return vec_env


# ===== æ¨¡å‹åŠ è½½/ä¿å­˜ =====


def get_model_path(stage: str, agent_type: str, model_dir: str):
    """è·å–æ¨¡å‹è·¯å¾„"""
    return os.path.join(model_dir, f"{stage}_{agent_type}_final.zip")


def load_previous_model(stage: str, agent_type: str, env, device: str, model_dir: str):
    """åŠ è½½ä¸Šä¸€é˜¶æ®µçš„æ¨¡å‹"""
    stage_config = TrainingConfig.get_stage_config(stage)

    # ç¡®å®šå‰ç½®æ¨¡å‹é˜¶æ®µ
    if agent_type == "hunter":
        prev_stage = stage_config.load_hunter_model
    else:  # prey
        prev_stage = stage_config.load_prey_model

    if prev_stage is None:
        return None

    model_path = get_model_path(prev_stage, agent_type, model_dir)
    if not os.path.exists(model_path):
        print(f"  âš ï¸  æ‰¾ä¸åˆ° {prev_stage} çš„ {agent_type} æ¨¡å‹: {model_path}")
        print(f"  å°†ä»å¤´å¼€å§‹è®­ç»ƒ {agent_type}")
        return None

    print(f"  âœ“ åŠ è½½ {prev_stage} çš„ {agent_type} æ¨¡å‹: {model_path}")
    model = PPO.load(model_path, env=env, device=device)
    return model


# ===== è®­ç»ƒå‡½æ•° =====


def train_stage(
    stage: str,
    model_dir: str = "curriculum_models",
    device: str = "auto",
    reward_log_interval: int = 10,
    n_envs: int = 4,
    use_subproc: bool = True,
    enable_hpo: bool = False,
):
    """
    è®­ç»ƒæŒ‡å®šé˜¶æ®µ

    Args:
        stage: è®­ç»ƒé˜¶æ®µ ("stage1", "stage2", "stage3", "stage4")
        model_dir: æ¨¡å‹ä¿å­˜ç›®å½•
        device: è®­ç»ƒè®¾å¤‡ ("auto", "cpu", "cuda")
        reward_log_interval: å¥–åŠ±æ—¥å¿—é—´éš”
        n_envs: å¹¶è¡Œç¯å¢ƒæ•°é‡
        use_subproc: æ˜¯å¦ä½¿ç”¨å¤šè¿›ç¨‹
        enable_hpo: æ˜¯å¦å¯ç”¨HPOå¢å¼º
    """
    # è·å–é˜¶æ®µé…ç½®
    stage_config = TrainingConfig.get_stage_config(stage)
    train_config = TrainingConfig()

    print("\n" + "=" * 80)
    print(f"{stage_config.name}")
    print(f"æè¿°: {stage_config.description}")
    print(f"æˆåŠŸæ ‡å‡†: {stage_config.success_criteria}")
    if enable_hpo:
        print("âœ¨ HPOå¢å¼º: å¯ç”¨")
    else:
        print("âš™ï¸  æ¨¡å¼: æ ‡å‡†è¯¾ç¨‹å­¦ä¹ ")
    print(f"è®­ç»ƒæ­¥æ•°: {stage_config.total_timesteps:,}")
    print(f"å­¦ä¹ ç‡: {stage_config.learning_rate}")
    print(f"å¹¶è¡Œç¯å¢ƒæ•°: {n_envs}")
    print("=" * 80)

    # åˆ›å»ºæ¨¡å‹ä¿å­˜ç›®å½•
    os.makedirs(model_dir, exist_ok=True)

    # åˆ›å»ºç¯å¢ƒ
    print("\nåˆ›å»ºç¯å¢ƒ...")
    vec_env = make_vec_env(
        stage=stage,
        model_dir=model_dir,
        n_envs=n_envs,
        use_subproc=use_subproc,
        enable_hpo=enable_hpo,
    )

    # è®­ç»ƒçŒäºº
    if stage_config.train_hunters:
        print("\nè®­ç»ƒçŒäºº...")

        # å°è¯•åŠ è½½å‰ä¸€é˜¶æ®µçš„æ¨¡å‹
        hunter_model = load_previous_model(stage, "hunter", vec_env, device, model_dir)

        if hunter_model is None:
            # ä»å¤´åˆ›å»ºæ¨¡å‹
            print("  âœ“ åˆ›å»ºæ–°çš„PPOæ¨¡å‹")
            hunter_model = PPO(
                "MlpPolicy",
                vec_env,
                learning_rate=stage_config.learning_rate,
                n_steps=train_config.PPO_N_STEPS,
                batch_size=train_config.PPO_BATCH_SIZE,
                n_epochs=train_config.PPO_N_EPOCHS,
                gamma=train_config.PPO_GAMMA,
                gae_lambda=train_config.PPO_GAE_LAMBDA,
                clip_range=train_config.PPO_CLIP_RANGE,
                ent_coef=train_config.PPO_ENT_COEF,
                verbose=1,
                device=device,
            )
        else:
            # æ›´æ–°å­¦ä¹ ç‡
            hunter_model.learning_rate = stage_config.learning_rate

        # åˆ›å»ºLoggerCallback
        callback = create_logger_callback(
            stage=f"{stage_config.name} [çŒäººè®­ç»ƒ]",
            total_steps=stage_config.total_timesteps,
            update_interval=100
        )

        # è®­ç»ƒ
        print(f"å¼€å§‹è®­ç»ƒçŒäºº ({stage_config.total_timesteps} æ­¥)...")
        hunter_model.learn(
            total_timesteps=stage_config.total_timesteps,
            callback=callback,
            log_interval=reward_log_interval,
            progress_bar=False,
        )

        # ä¿å­˜
        hunter_path = get_model_path(stage, "hunter", model_dir)
        hunter_model.save(hunter_path)
        print(f"âœ… çŒäººæ¨¡å‹å·²ä¿å­˜: {hunter_path}")

    # è®­ç»ƒçŒç‰©
    if stage_config.train_prey:
        print("\nè®­ç»ƒçŒç‰©...")

        # å°è¯•åŠ è½½å‰ä¸€é˜¶æ®µçš„æ¨¡å‹
        prey_model = load_previous_model(stage, "prey", vec_env, device, model_dir)

        if prey_model is None:
            print("  âœ“ åˆ›å»ºæ–°çš„PPOæ¨¡å‹")
            prey_model = PPO(
                "MlpPolicy",
                vec_env,
                learning_rate=stage_config.learning_rate,
                n_steps=train_config.PPO_N_STEPS,
                batch_size=train_config.PPO_BATCH_SIZE,
                n_epochs=train_config.PPO_N_EPOCHS,
                gamma=train_config.PPO_GAMMA,
                gae_lambda=train_config.PPO_GAE_LAMBDA,
                clip_range=train_config.PPO_CLIP_RANGE,
                ent_coef=train_config.PPO_ENT_COEF,
                verbose=1,
                device=device,
            )
        else:
            prey_model.learning_rate = stage_config.learning_rate

        # åˆ›å»ºLoggerCallback
        callback = create_logger_callback(
            stage=f"{stage_config.name} [çŒç‰©è®­ç»ƒ]",
            total_steps=stage_config.total_timesteps,
            update_interval=100
        )

        print(f"å¼€å§‹è®­ç»ƒçŒç‰© ({stage_config.total_timesteps} æ­¥)...")
        prey_model.learn(
            total_timesteps=stage_config.total_timesteps,
            callback=callback,
            log_interval=reward_log_interval,
            progress_bar=False,
        )

        prey_path = get_model_path(stage, "prey", model_dir)
        prey_model.save(prey_path)
        print(f"âœ… çŒç‰©æ¨¡å‹å·²ä¿å­˜: {prey_path}")

    vec_env.close()
    print(f"\nâœ… {stage.upper()} è®­ç»ƒå®Œæˆ!")
    print(f"æˆåŠŸæ ‡å‡†: {stage_config.success_criteria}")
    print("è¯·è¯„ä¼°æ¨¡å‹æ˜¯å¦è¾¾åˆ°æ ‡å‡†ï¼Œå†è¿›å…¥ä¸‹ä¸€é˜¶æ®µã€‚\n")


# ===== ä¸»å‡½æ•° =====


def main():
    parser = argparse.ArgumentParser(
        description="è¯¾ç¨‹å­¦ä¹ è®­ç»ƒè„šæœ¬ (ç»Ÿä¸€ç‰ˆæœ¬)",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ä½¿ç”¨ç¤ºä¾‹:
  # è®­ç»ƒå•ä¸ªé˜¶æ®µ (æ ‡å‡†æ¨¡å¼)
  python train_curriculum.py --stage stage1

  # è®­ç»ƒå•ä¸ªé˜¶æ®µ (å¯ç”¨HPOå¢å¼º)
  python train_curriculum.py --stage stage1 --enable_hpo

  # è®­ç»ƒæ‰€æœ‰é˜¶æ®µ
  python train_curriculum.py --stages stage1 stage2 stage3 stage4

  # é«˜æ€§èƒ½è®­ç»ƒ (HPO + å¤šè¿›ç¨‹)
  python train_curriculum.py --stages stage1 stage2 stage3 stage4 \\
      --enable_hpo --n_envs 8 --device cpu

  # å¯¹æ¯”å®éªŒ
  python train_curriculum.py --stage stage1                  # åŸºçº¿
  python train_curriculum.py --stage stage1 --enable_hpo    # HPOç‰ˆæœ¬
        """,
    )

    parser.add_argument(
        "--stage",
        type=str,
        choices=["stage1", "stage2", "stage3", "stage4"],
        help="è®­ç»ƒå•ä¸ªé˜¶æ®µ",
    )

    parser.add_argument(
        "--stages",
        type=str,
        nargs="+",
        choices=["stage1", "stage2", "stage3", "stage4"],
        help="è®­ç»ƒå¤šä¸ªé˜¶æ®µ (æŒ‰é¡ºåº)",
    )

    parser.add_argument(
        "--model_dir",
        type=str,
        default="curriculum_models",
        help="æ¨¡å‹ä¿å­˜ç›®å½• (é»˜è®¤: curriculum_models)",
    )

    parser.add_argument(
        "--device",
        type=str,
        default="auto",
        choices=["cpu", "cuda", "auto"],
        help="è®¾å¤‡ (é»˜è®¤: auto)",
    )

    parser.add_argument(
        "--n_envs",
        type=int,
        default=4,
        help="å¹¶è¡Œç¯å¢ƒæ•°é‡ (é»˜è®¤: 4)",
    )

    parser.add_argument(
        "--no_subproc",
        action="store_true",
        help="ç¦ç”¨å¤šè¿›ç¨‹ (ä½¿ç”¨DummyVecEnv)",
    )

    parser.add_argument(
        "--enable_hpo",
        action="store_true",
        help="å¯ç”¨HPOå¢å¼º (è‡ªé€‚åº”æƒé‡ + å¯¹æŠ—å¹³è¡¡ + è·ç¦»è¿½è¸ª)",
    )

    parser.add_argument(
        "--reward_log_interval",
        type=int,
        default=10,
        help="å¥–åŠ±æ—¥å¿—é—´éš” (é»˜è®¤: 10)",
    )

    args = parser.parse_args()

    # ç¡®å®šè®­ç»ƒé˜¶æ®µ
    stages_to_train = []
    if args.stage:
        stages_to_train = [args.stage]
    elif args.stages:
        stages_to_train = args.stages
    else:
        # é»˜è®¤ï¼šè®­ç»ƒæ‰€æœ‰é˜¶æ®µ
        stages_to_train = ["stage1", "stage2", "stage3", "stage4"]

    # è®­ç»ƒå¾ªç¯
    print("\n" + "=" * 80)
    print("è¯¾ç¨‹å­¦ä¹ è®­ç»ƒ")
    if args.enable_hpo:
        print("âœ¨ HPOå¢å¼ºæ¨¡å¼")
    else:
        print("âš™ï¸  æ ‡å‡†æ¨¡å¼")
    print(f"è®­ç»ƒé˜¶æ®µ: {', '.join(stages_to_train)}")
    print("=" * 80)

    start_time = datetime.now()

    for stage in stages_to_train:
        try:
            train_stage(
                stage=stage,
                model_dir=args.model_dir,
                device=args.device,
                reward_log_interval=args.reward_log_interval,
                n_envs=args.n_envs,
                use_subproc=not args.no_subproc,
                enable_hpo=args.enable_hpo,
            )
        except KeyboardInterrupt:
            print(f"\nè®­ç»ƒåœ¨ {stage} é˜¶æ®µè¢«ä¸­æ–­")
            break

    end_time = datetime.now()
    elapsed = (end_time - start_time).total_seconds()

    print("\n" + "=" * 80)
    print("ğŸ‰ è®­ç»ƒå®Œæˆ!")
    print("=" * 80)
    print(f"æ€»è€—æ—¶: {elapsed / 60:.1f} åˆ†é’Ÿ")
    print(f"æ¨¡å‹ä¿å­˜åœ¨: {args.model_dir}/")
    print("\nè¿è¡Œå¯è§†åŒ–æ¼”ç¤º:")
    print(f"  python main.py")
    print(f"\nè¿è¡Œæ¨¡å‹æ¼”ç¤º:")
    print(f"  python demo_curriculum_models.py")
    print()


if __name__ == "__main__":
    main()
